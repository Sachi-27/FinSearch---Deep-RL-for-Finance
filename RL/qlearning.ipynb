{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Data collection and saving complete.\n"
     ]
    }
   ],
   "source": [
    "# nifty50_stocks = [\n",
    "#     \"RELIANCE.NS\" , \"TCS.NS\", \"HDFCBANK.NS\",  # ... Add other stock symbols here\n",
    "#     # List of all NIFTY50 stocks: https://www1.nseindia.com/live_market/dynaContent/live_watch/equities_stock_watch.htm\n",
    "# ]\n",
    "\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2019-06-30\"\n",
    "\n",
    "stock_data = pd.DataFrame()\n",
    "\n",
    "stock_data = yf.download(\"RELIANCE.NS\", start=start_date, end=end_date)\n",
    "stock_data.to_csv('reliance_stock_data.csv', index=False)\n",
    "\n",
    "print(\"Data collection and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530.323059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538.891846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>547.832092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>546.395691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close\n",
       "0  532.700500\n",
       "1  530.323059\n",
       "2  538.891846\n",
       "3  547.832092\n",
       "4  546.395691"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the csv file, read only the column named Close\n",
    "stock_data = pd.read_csv(\"reliance_stock_data.csv\")[[\"Close\"]]\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2339.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.935741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>255.480789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>334.875702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>427.250092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.683289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>555.992249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1395.620850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close\n",
       "count  2339.000000\n",
       "mean    586.935741\n",
       "std     255.480789\n",
       "min     334.875702\n",
       "25%     427.250092\n",
       "50%     489.683289\n",
       "75%     555.992249\n",
       "max    1395.620850"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2105, 234)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide into 90% training and 10% testing\n",
    "training_stock_data = stock_data[:int(len(stock_data)*0.9)]\n",
    "testing_stock_data = stock_data[int(len(stock_data)*0.9):]\n",
    "\n",
    "len(training_stock_data), len(testing_stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Temporal Difference Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Q_Learning_Agent:\n",
    "    def __init__(self, num_iterations=200, checkpoint=10):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        self.epsilon = 0.9\n",
    "        self.decay = 0.999\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.9\n",
    "\n",
    "        self.max_lots_tradable = 36\n",
    "        self.lot_size = 50\n",
    "        self.max_lots_cumulative_traded = 100\n",
    "        self.transaction_cost = 0.0000335\n",
    "\n",
    "        self.action_space = [i for i in range(-self.max_lots_tradable, self.max_lots_tradable+1)]\n",
    "        self.state_space = [i for i in range(-self.max_lots_cumulative_traded, self.max_lots_cumulative_traded+1)]\n",
    "        self.q_table = np.zeros((len(self.state_space), len(self.action_space)))\n",
    "\n",
    "        self.current_state = 0\n",
    "        self.cumulative_reward = 0\n",
    "    \n",
    "    def get_action(self):\n",
    "        if np.random.uniform(0,1) <= self.epsilon:\n",
    "            while True:\n",
    "                action = np.random.choice(self.action_space)\n",
    "                if (self.current_state + action) in self.state_space:\n",
    "                    self.epsilon = self.epsilon * self.decay\n",
    "                    return action\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            return np.argmax(self.q_table[self.current_state])\n",
    "    \n",
    "    def update_state(self, action):\n",
    "        self.current_state += action\n",
    "    \n",
    "    def get_reward(self, action, old_val, new_val):\n",
    "        reward = -self.current_state*old_val*self.lot_size\n",
    "        self.update_state(action)\n",
    "        reward += self.current_state*new_val*self.lot_size\n",
    "        reward -= abs(action)*self.transaction_cost\n",
    "        return reward\n",
    "\n",
    "    def update_q_table(self, action, old_val, new_val):\n",
    "        q_old = self.q_table[self.current_state][action]\n",
    "        reward = self.get_reward(action, old_val, new_val)\n",
    "        self.cumulative_reward += reward\n",
    "        q_new = reward + self.gamma*np.max(self.q_table[self.current_state])\n",
    "        self.q_table[self.current_state-action][action] = (1-self.alpha)*q_old + self.alpha*q_new\n",
    "\n",
    "    def train(self, data):\n",
    "        cumulative_rewards = []\n",
    "        for i in range(self.num_iterations):\n",
    "            print(\"Iteration: \", i+1)\n",
    "            prev_val = data.values[0]\n",
    "            for val in tqdm(data.values[1:]):\n",
    "                action = self.get_action()\n",
    "                self.update_q_table(action, prev_val, val)\n",
    "                prev_val = val\n",
    "            if (i+1)%self.checkpoint == 0:\n",
    "                print(\"Iteration: \", i+1, \" Cumulative Reward: \", self.cumulative_reward)\n",
    "            cumulative_rewards.append(self.cumulative_reward)\n",
    "            self.cumulative_reward = 0\n",
    "            self.epsilon = 0.9\n",
    "            self.current_state = 0\n",
    "        return cumulative_rewards\n",
    "\n",
    "    def plot_progress(self, cumulative_rewards):\n",
    "        plt.plot(cumulative_rewards)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cumulative Reward\")\n",
    "        plt.show()\n",
    "    \n",
    "    def save_q_table(self, filename):\n",
    "        np.save(filename, self.q_table)\n",
    "    \n",
    "    def load_q_table(self, filename):\n",
    "        self.q_table = np.load(filename)\n",
    "    \n",
    "    def test(self, data):\n",
    "        self.current_state = 0\n",
    "        self.cumulative_reward = 0\n",
    "        actions, rewards = [], []\n",
    "        prev_val = data.values[0]\n",
    "        for val in data.values[1:]:\n",
    "            action = np.argmax(self.q_table[self.current_state])\n",
    "            actions.append(action)\n",
    "            reward = self.get_reward(action, prev_val, val)\n",
    "            rewards.append(reward)\n",
    "            self.cumulative_reward += reward\n",
    "            prev_val = val\n",
    "        return actions, rewards, self.cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 106/2104 [01:51<35:02,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[39m=\u001b[39m Q_Learning_Agent(num_iterations \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m, checkpoint \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m cumulative_rewards \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(training_stock_data)\n\u001b[0;32m      3\u001b[0m agent\u001b[39m.\u001b[39mplot_progress(cumulative_rewards)\n",
      "Cell \u001b[1;32mIn[20], line 60\u001b[0m, in \u001b[0;36mQ_Learning_Agent.train\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     58\u001b[0m prev_val \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m tqdm(data\u001b[39m.\u001b[39mvalues[\u001b[39m1\u001b[39m:]):\n\u001b[1;32m---> 60\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_action()\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_q_table(action, prev_val, val)\n\u001b[0;32m     62\u001b[0m     prev_val \u001b[39m=\u001b[39m val\n",
      "Cell \u001b[1;32mIn[20], line 28\u001b[0m, in \u001b[0;36mQ_Learning_Agent.get_action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[0;32m     27\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m         action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space)\n\u001b[0;32m     29\u001b[0m         \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state \u001b[39m+\u001b[39m action) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_space:\n\u001b[0;32m     30\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Q_Learning_Agent(num_iterations = 200, checkpoint = 10)\n",
    "cumulative_rewards = agent.train(training_stock_data)\n",
    "agent.plot_progress(cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m actions, rewards, cumulative_reward \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtest(testing_stock_data)\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(actions)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "actions, rewards, cumulative_reward = agent.test(testing_stock_data)\n",
    "plt.plot(actions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mplot(rewards)\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cumulative_reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCumulative Reward :\u001b[39m\u001b[39m\"\u001b[39m, cumulative_reward)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cumulative_reward' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Cumulative Reward :\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
